# -*- coding: utf-8 -*-
"""Credit_card_application_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gf2UG6G2P9xlpPF1-JwdmzEui1nC_sTV
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/gdrive')

df=pd.read_csv('/content/gdrive/MyDrive/BDS_practice_data/application_data.csv')

df.head(20)

"""1) Find all categorical variables

2) Find Count of instances in 0 and 1 class in TARGET column

3) Find % of instances in 0 and 1 class in TARGET column

4) Bar chart for both 2nd and 3rd

5) Find unique values in each column

6) Find how many nan values in each column

7) Preprocessing for each column differently according to the nan values in each column

8) As all remaining columns with missing value above 40% are normalized we need to remove them as their is no way to substitute values in them

9) Analysis of OCCUPATION_TYPE do a plot showing count of each category

10) which category comes the most in OCCUPATION_TYPE  column

11) How many TARGET COLUMNS are 1 and 0 for most reoccurring category in OCCUPATION_TYPE   column

12) How many TARGET COLUMNS are 1 and have OCCUPATION_TYPE  column  as labourer

13) pie chart for counts of NAME_CONTRACT_TYPE and NAME_INCOME_TYPE

14) Sample Homework

a) Remove  occupation type column and fill remaining nan values with mode or median and apply

Random Forest, ANN,KNN,SVM and logistic regression and compare results using a bar chart for each model

1) Find all categorical variables
"""

col_types=df.dtypes

col_types

categorical_columns=col_types[col_types=='object'].index
categorical_columns

"""2) Find Count of instances in 0 and 1 class in TARGET column"""

target_value_count=df.TARGET.value_counts()
target_value_count

"""3) Find % of instances in 0 and 1 class in TARGET column"""

target_value_percentage=(df.TARGET.value_counts()/len(df.index))*100
target_value_percentage

"""4) Bar chart for both 2nd and 3rd"""

list(target_value_count.index)

plt.bar(target_value_count.index, target_value_count.values)
plt.xticks([0,1])
plt.ylabel("Number of Observations")
plt.title("Count of Observations in Target Class")

plt.bar(target_value_percentage.index, target_value_percentage.values)
plt.xticks([0,1])
plt.ylabel("Percentage of Observations")
plt.title("Percentage of Observations in Target Class")

"""5) Find # of unique values in each column"""

df.CODE_GENDER.unique()

for i in df.columns:
  print(i, df[i].unique().shape[0])

"""6) Find how many nan values in each column"""

df.isna().sum()[:15]

(df.isna().sum()/len(df.index)*100)[:15]

"""7) Preprocessing for each column differently according to the nan values in each column

8) As all remaining columns with missing value above 40% are normalized we need to remove them as their is no way to substitute values in them
"""

df.isna().sum()['AMT_ANNUITY']

#Find the name of the columns above 40% missing values and remove them from data frame
col=list(df.columns)
print('Number of missing values in each col')
for i in range(len(df.columns)):
  if df[col[i]].isnull().sum()/len(df.index)*100>40:
    print(col[i],'=',df[col[i]].isnull().sum()/len(df.index)*100)
    df.drop(col[i],axis=1,inplace=True)
print(len(df.columns))

col_types=df.dtypes

cat_col=[]
num_col=[]
for i in range(len(col_types)):
  if col_types[i]=='object':
    cat_col.append(col_types.index[i])
  else:
    num_col.append(col_types.index[i])
print(len(cat_col))
print(len(num_col))

## filling nan values of all num columns with respective mean
for i in num_col:
  #print(i)
  if df.isna().sum()[i]>0:
    df[i].fillna(df[i].mean(), inplace=True)
df.isna().sum()[:15]

df.NAME_TYPE_SUITE.mode()[0]

"""Handling occupation type before applying model these three methods need to be done

1st way to drop occupation type column itself

2nd drop rows which are nan in occupation type column

3rd Introduce a new a category called unkown and create dummies
"""

## using option 3rd
df.OCCUPATION_TYPE = df['OCCUPATION_TYPE'].fillna('Unknown')

## filling nan values of all categorical columns with respective mode values
for i in cat_col:
  #print(i)
  if df.isna().sum()[i]>0:
    df[i].fillna(df[i].mode()[0], inplace=True)
df.isna().sum()[:15]

"""9) Analysis of OCCUPATION_TYPE do a plot showing count of each category

10) which category comes the most in OCCUPATION_TYPE column
"""

df.OCCUPATION_TYPE.value_counts(ascending=True)

plt.bar(df.OCCUPATION_TYPE.value_counts().index, df.OCCUPATION_TYPE.value_counts().values)
plt.xticks(rotation=90)
plt.title('Count of each occupation type')
plt.show

## top 5 categories in Occupation types
df.OCCUPATION_TYPE.value_counts()[0:4]

"""11) How many TARGET COLUMNS are 1 and 0 for most reoccurring category in OCCUPATION_TYPE column"""

##getting most recurring category in occupation type i.e
df.OCCUPATION_TYPE.value_counts().index[0]
df[df.OCCUPATION_TYPE==df.OCCUPATION_TYPE.value_counts().index[0]].TARGET.value_counts()

"""12) How many TARGET COLUMNS are 1 and have OCCUPATION_TYPE column as labourer"""

df.groupby('OCCUPATION_TYPE').TARGET.value_counts()

df.groupby('OCCUPATION_TYPE').TARGET.value_counts()[('Laborers',1)]

"""13) pie chart for counts of NAME_CONTRACT_TYPE and NAME_INCOME_TYPE"""

df.NAME_CONTRACT_TYPE.value_counts().index

(df.NAME_CONTRACT_TYPE.value_counts().values)
plt.pie(df.NAME_CONTRACT_TYPE.value_counts().values, labels=df.NAME_CONTRACT_TYPE.value_counts().index, autopct='%1.1f%%')
plt.title('Counts of unique categories in NAME_CONTRACT_TYPE')

df.NAME_INCOME_TYPE.value_counts()
plt.pie(df.NAME_INCOME_TYPE.value_counts().values, labels=df.NAME_INCOME_TYPE.value_counts().index, autopct='%1.1f%%')
plt.tight_layout()
plt.show()

"""Random Forest, ANN,KNN,SVM and logistic regression and compare results using a bar chart for each model

Random Forest
"""

pd.set_option('display.max_columns', None)
df.head(10)

"""one hot encoding the categorical variables"""

cat_col

cat_col_index=[]
for i in range(len(df.columns)):
  if df.columns[i] in cat_col:
    cat_col_index.append(i)
cat_col_index

x=df.drop(['SK_ID_CURR','TARGET'], axis=1)

cat_col_index=[]
for i in range(len(x.columns)):
  if x.columns[i] in cat_col:
    cat_col_index.append(i)
cat_col_index

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
ct=ColumnTransformer(transformers=[('OneHotEncoder',OneHotEncoder(),[0, 1, 2, 3, 9, 10, 11, 12, 13, 25, 29, 37])], remainder='passthrough')
X=ct.fit_transform(np.array(x))

X[0]

y=df.TARGET.values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=2, train_size=0.75)

from sklearn.ensemble import RandomForestClassifier
rf_model=RandomForestClassifier(n_estimators=30, random_state=2)
rf_model.fit(X_train,y_train)
y_rf_predict=rf_model.predict(X_test)

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix
print('f1 score = ',f1_score(y_test,y_rf_predict))
print('accuracy score = ',accuracy_score(y_test,y_rf_predict))
print('precision score = ',precision_score(y_test,y_rf_predict))
print('recall score = ',recall_score(y_test,y_rf_predict))
print(confusion_matrix(y_test,y_rf_predict))

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn_model=KNeighborsClassifier(n_neighbors=100)
knn_model.fit(X_train,y_train)
y_knn_predict=knn_model.predict(X_test)

y_knn_predict.shape

print('f1 score = ',f1_score(y_test,y_knn_predict))
print('accuracy score = ',accuracy_score(y_test,y_knn_predict))
print('precision score = ',precision_score(y_test,y_knn_predict, zero_division=1))
print('recall score = ',recall_score(y_test,y_knn_predict))
print(confusion_matrix(y_test,y_knn_predict))

"""Logistic Regression"""

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
Xsc=sc.fit_transform(X)

from sklearn.model_selection import train_test_split
Xsc_train, Xsc_test, y_train, y_test=train_test_split(Xsc, y, random_state=1, test_size=0.25)

from sklearn.linear_model import LogisticRegression
logreg_model=LogisticRegression()
logreg_model.fit(Xsc_train,y_train)
y_log_predict=logreg_model.predict(Xsc_test)

"""SVM"""

from sklearn.svm import SVC
svm_model = SVC(kernel = 'rbf')
svm_model.fit(Xsc_train, y_train)
y_svm_predict=svm_model.predict(Xsc_test)

print('f1 score = ',f1_score(y_test,y_knn_predict))
print('accuracy score = ',accuracy_score(y_test,y_knn_predict))
print('precision score = ',precision_score(y_test,y_knn_predict, zero_division=1))
print('recall score = ',recall_score(y_test,y_knn_predict))
print(confusion_matrix(y_test,y_knn_predict))

"""ANN"""

import tensorflow as tf

ann = tf.keras.models.Sequential()
##adding layers
ann.add(tf.keras.layers.Dense(units=10000, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1000, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
##compiling
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
#training
ann.fit(Xsc_train, y_train, batch_size = 32, epochs = 5)