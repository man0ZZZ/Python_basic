# -*- coding: utf-8 -*-
"""AirBnB_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x2XGC3j6xyN6GAIRngjpLE1TMDznzH1q
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/gdrive')

df_abnb=pd.read_csv('/content/gdrive/MyDrive/BDS_practice_data/AB_NYC_2019.csv')
df_abnb.head(10)

df_abnb

"""1) Calculate How many properties in each neighbour hood group

2) Which Neighbour Hood Group Has highest prices use bar chart to show this for all neighbourhood groups

3) Which Neighboorhood group has lowest prices use bar chart to show this for all neighbourhood groups

4) Highest and lowest price airbnb in each Neighbour Hood Group

5) Which Host has Highest count of properties

6) Which neighbourhood is most expensive and cheapest top 10 barchart

7) Count different type of room_type and do a pie chart showing % of each room type in overall dataset

8) scatter plot between longitutude and latitude for different neighbourhood groups

9) scatter plot between longitutude and latitude for different room_types

11)  room_type vs price bar chart for the mean and highest and lowest


"""

#1) Calculate How many properties in each neighbour hood group

df_abnb.neighbourhood_group.value_counts()

df_abnb.isna().sum()

#2) Which Neighbour Hood Group Has highest prices use bar chart to show this for all neighbourhood groups
bar_max=df_abnb.groupby('neighbourhood_group').price.max()

bar_max.values

plt.bar(bar_max.index, bar_max.values)

# 3) Which Neighboorhood group has lowest prices use bar chart to show this for all neighbourhood groups
bar_min=df_abnb.groupby('neighbourhood_group').price.min()
plt.bar(bar_min.index, bar_min.values)

#4) Highest and lowest price airbnb in each Neighbour Hood Group
pd.DataFrame({'max':bar_max, 'min':bar_min})

#5) Which Host has Highest count of properties

#6) Which neighbourhood is most expensive and cheapest top 10 barchart
bar_exp=df_abnb.groupby('neighbourhood').price.max()

bar_exp=bar_exp.sort_values(ascending=False)[:10]

bar_cheap=df_abnb.groupby('neighbourhood').price.min()
bar_cheap=bar_cheap.sort_values()[:10]

plt.bar(bar_cheap.index,bar_cheap.values)
plt.xticks(rotation=45)
plt.show()

plt.bar(bar_exp.index,bar_exp.values)
plt.xticks(rotation=45)
plt.show()

#7) Count different type of room_type and do a pie chart showing % of each room type in overall dataset
pie=(df_abnb.groupby('room_type').room_type.count())/48895*100

plt.pie(pie.values, labels=pie.index, autopct='%1.1f%%')
plt.show()

#8) scatter plot between longitutude and latitude for different neighbourhood groups
df_abnb_grouped=df_abnb.groupby('neighbourhood_group')

print(len(df_abnb_grouped))

for i in df_abnb_grouped: ## iterate over the grouped df.. which is actually a tuple with 0 index holding the grouped values
## and 1 index holding the data frame of the resulting subset of main data.... and then you can access individual column after
## getting into the df which will be i[1]
  print(i[1].latitude)

for i in df_abnb_grouped:
  plt.scatter(i[1].longitude, i[1].latitude)
  plt.title(i[0])
  plt.show()

#9) scatter plot between longitutude and latitude for different room_types
df_abnb_room=df_abnb.groupby('room_type')
print(len(df_abnb_room))

plt.rcParams['lines.markersize'] = 1 ##fix the size of the dot in matplotlib globally
for i in df_abnb_room:
  plt.scatter(i[1].latitude,i[1].longitude)
  plt.title(i[0])
  plt.show()

df_abnb.isna().sum()

#11)  room_type vs price bar chart for the mean and highest and lowest
room_v_price_mean=df_abnb.groupby('room_type').price.mean()
plt.bar(room_v_price_mean.index, room_v_price_mean.values)

room_v_price_max=df_abnb.groupby('room_type').price.max()
plt.bar(room_v_price_max.index, room_v_price_max.values)

room_v_price_min=df_abnb.groupby('room_type').price.min()
plt.bar(room_v_price_min.index, room_v_price_min.values)

"""Preprocessing"""

df_abnb.columns

len(df_abnb.index)

len(df_abnb.host_id.unique())

#drop last_review and name column
df_abnb_final=df_abnb.drop(['last_review','name','id','host_id','host_name'], axis=1)
df_abnb_final.columns

df_abnb_final.isna().sum()

#fill nan values
df_abnb_final['reviews_per_month']=df_abnb_final.reviews_per_month.fillna(df_abnb_final.reviews_per_month.mean())
df_abnb_final.isna().sum()

col_types=df_abnb_final.dtypes
col_cat_index=[]
for i in range(len(col_types.index)):
  if col_types[i]=='object':
    col_cat_index.append(i)

print(col_cat_index)

x=df_abnb_final.drop(labels=['price'], axis=1)
y=df_abnb_final['price'].values

len(x.neighbourhood.unique())

indi=[0,1,4]

x.columns[[0,1,4]]

#simple way of ohe
## take only the categorical col and concatenate at the end
ind=[0,1,4]
x_num=x.drop(x.columns[ind], axis=1)
x_cat=x.iloc[:,ind]
from sklearn.preprocessing import OneHotEncoder
ohe=OneHotEncoder()
x_cat.head(5)
#x_cat=ohe.fit_transform(x_cat) ## gives sparse matrix as output
## to convert it to normal array...use .toarray()
x_cat=ohe.fit_transform(x_cat).toarray()

x_cat[0]

##one hot encode
from sklearn.compose import ColumnTransformer # sklearn.compose.ColumnTransformer
from sklearn.preprocessing import OneHotEncoder # sklearn.preprocessing.OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0,1,4])], remainder='passthrough')
x1 = ct.fit_transform(x).toarray() ### very important otherwise converts to sparse matrix... .toarray()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test =train_test_split(x1, y, random_state=1, test_size=0.25)

print(x_test[1])

"""Random forest"""

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=20, random_state=2)
rf_model.fit(x_train, y_train)

y_pred=rf_model.predict(x_test)

from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, f1_score, recall_score
print(f1_score(y_test, y_pred, average='weighted'))
print(accuracy_score(y_test, y_pred))# Display F1 score
print(recall_score(y_test,y_pred,average='weighted'))
print(precision_score(y_test,y_pred,average='weighted'))

"""Decision Tree"""

from sklearn.tree import DecisionTreeRegressor
dt_model=DecisionTreeRegressor(random_state=4)
dt_model.fit(x_train, y_train)

y_dt_pred=dt_model.predict(x_test)

y_dt_pred[:10]

y_dt_pred.shape

print(f1_score(y_test, y_dt_pred, average='weighted'))
print(accuracy_score(y_test, y_dt_pred))# Display F1 score
print(recall_score(y_test,y_dt_pred,average='macro'))
print(precision_score(y_test,y_dt_pred,average='macro'))

"""SVM"""

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_s=sc.fit_transform(x1) ##
y_s=sc.fit_transform(y.reshape(-1,1))

from sklearn.model_selection import train_test_split
x_s_train, x_s_test, y_s_train, y_s_test = train_test_split(x_s, y_s, random_state=2, test_size=0.25)

from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf') ##three kernels are available,,, linear, polynomial, and radial basis function(rbf)
regressor.fit(x_s_train, y_s_train.ravel())

y_svm_pred=regressor.predict(x_s_test)

y_svm_pred[:10]

#rescale the predicted value to original scale
y_s_pred = sc.inverse_transform(y_svm_pred.reshape(-1,1))

np.concatenate((y_test[0:10].reshape(10,1), y_s_pred[0:10].ravel().reshape(10,1)), axis=1)

y_s_pred=y_s_pred.ravel()

y_s_pred.shape

y_test.shape

#print(f1_score(y_test, y_svm_pred, average='weighted'))
print(accuracy_score(y_test, y_s_pred))# Display F1 score
print(recall_score(y_test,y_s_pred,average='weighted'))
print(precision_score(y_test,y_s_pred,average='weighted'))